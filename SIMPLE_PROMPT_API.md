# ü§ñ Endpoint Simple de VertexAI

## Descripci√≥n

Endpoint optimizado para enviar prompts simples a VertexAI y recibir respuestas estructuradas en formato JSON.

## üìç Endpoint

```
POST /vertex-ai/prompt
```

## üìù Request Body

### Estructura

```json
{
  "prompt": "Tu pregunta o solicitud aqu√≠",
  "temperature": 0.7, // Opcional: 0.0 - 2.0
  "maxTokens": 1024 // Opcional: 1 - 8192
}
```

### Campos

- **`prompt`** (requerido): El texto que quieres enviar a la IA
- **`temperature`** (opcional): Controla la creatividad de la respuesta
  - `0.0`: Respuestas muy predecibles y consistentes
  - `1.0`: Balance entre creatividad y coherencia
  - `2.0`: M√°xima creatividad (puede ser impredecible)
- **`maxTokens`** (opcional): M√°ximo n√∫mero de tokens en la respuesta (1-8192)

## üìä Response Structure

### Respuesta Exitosa

```json
{
  "success": true,
  "data": {
    "response": "Respuesta generada por la IA",
    "model": "gemini-1.5-flash",
    "promptTokens": 25,
    "responseTokens": 150,
    "totalTokens": 175,
    "temperature": 0.7,
    "processingTime": "1250ms"
  },
  "metadata": {
    "requestId": "req_1728123456789_abc123",
    "timestamp": "2025-10-04T14:30:45.123Z",
    "version": "1.0.0"
  }
}
```

### Respuesta con Error

```json
{
  "success": false,
  "data": {
    "response": "",
    "model": "gemini-1.5-flash",
    "temperature": 0.7,
    "processingTime": "500ms"
  },
  "metadata": {
    "requestId": "req_1728123456789_def456",
    "timestamp": "2025-10-04T14:30:45.123Z",
    "version": "1.0.0"
  },
  "error": {
    "code": "GENERATION_ERROR",
    "message": "Descripci√≥n del error",
    "details": "Detalles t√©cnicos del error"
  }
}
```

## üöÄ Ejemplos de Uso

### 1. Pregunta Simple

```bash
curl -X POST http://localhost:3000/vertex-ai/prompt \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "¬øQu√© es la inteligencia artificial?"
  }'
```

### 2. Con Par√°metros Personalizados

```bash
curl -X POST http://localhost:3000/vertex-ai/prompt \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Escribe un poema sobre la tecnolog√≠a",
    "temperature": 0.9,
    "maxTokens": 200
  }'
```

### 3. Generar Datos Estructurados

```bash
curl -X POST http://localhost:3000/vertex-ai/prompt \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Genera un objeto JSON con informaci√≥n de un producto: nombre, precio, categor√≠a y descripci√≥n",
    "temperature": 0.3,
    "maxTokens": 300
  }'
```

### 4. Lista de Elementos

```bash
curl -X POST http://localhost:3000/vertex-ai/prompt \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Dame 5 consejos para ser m√°s productivo trabajando desde casa",
    "temperature": 0.6,
    "maxTokens": 400
  }'
```

## üìà Casos de Uso

### ‚úÖ Ideal para:

- Preguntas y respuestas r√°pidas
- Generaci√≥n de contenido creativo
- Creaci√≥n de datos de prueba
- Explicaciones y tutoriales
- Generaci√≥n de c√≥digo
- Traducci√≥n de textos
- Res√∫menes de informaci√≥n

### üéØ Ejemplos Pr√°cticos:

#### Generaci√≥n de C√≥digo

```json
{
  "prompt": "Crea una funci√≥n en JavaScript que valide si un email es v√°lido",
  "temperature": 0.2,
  "maxTokens": 300
}
```

#### Datos de Prueba

```json
{
  "prompt": "Genera un array JSON de 5 usuarios ficticios con nombre, email, edad y ciudad",
  "temperature": 0.5,
  "maxTokens": 500
}
```

#### Explicaciones T√©cnicas

```json
{
  "prompt": "Explica qu√© son las APIs REST y c√≥mo funcionan, en t√©rminos simples",
  "temperature": 0.4,
  "maxTokens": 600
}
```

#### Contenido Creativo

```json
{
  "prompt": "Escribe una historia corta de ciencia ficci√≥n sobre robots que aprenden a sentir emociones",
  "temperature": 0.8,
  "maxTokens": 800
}
```

## üîß Par√°metros de Configuraci√≥n

### Temperature Guidelines

- **0.0 - 0.3**: Para tareas que requieren precisi√≥n (c√≥digo, datos, f√≥rmulas)
- **0.4 - 0.7**: Para contenido equilibrado (explicaciones, documentaci√≥n)
- **0.8 - 1.0**: Para contenido creativo (historias, ideas)
- **1.1 - 2.0**: Para m√°xima creatividad (experimental)

### MaxTokens Guidelines

- **50-100**: Respuestas muy cortas
- **100-300**: Respuestas medianas
- **300-600**: Respuestas largas
- **600-1024**: Respuestas muy extensas
- **1024+**: Contenido muy largo (art√≠culos, historias)

## ‚ö†Ô∏è Consideraciones

1. **Rate Limiting**: El endpoint puede tener l√≠mites de uso
2. **Timeout**: Requests muy complejos pueden tardar m√°s tiempo
3. **Costos**: Cada request consume tokens que pueden tener costo
4. **Contenido**: Evita prompts que generen contenido inapropiado

## üîç Debugging

### Usar el Request ID

Cada respuesta incluye un `requestId` √∫nico que puedes usar para tracking:

```json
"metadata": {
  "requestId": "req_1728123456789_abc123"
}
```

### Monitorear Processing Time

```json
"data": {
  "processingTime": "1250ms"
}
```

### Token Usage

```json
"data": {
  "promptTokens": 25,
  "responseTokens": 150,
  "totalTokens": 175
}
```

## üß™ Script de Pruebas

Ejecuta el script de pruebas incluido:

```bash
node test-simple-prompt.js
```
